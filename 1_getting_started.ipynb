{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "view-in-github"
   },
   "source": [
    "<a href=\"https://colab.research.google.com/github/araffin/rl-tutorial-jnrr19/blob/sb3/1_getting_started.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "hyyN-2qyK_T2"
   },
   "source": [
    "# Stable Baselines3 Tutorial - Getting Started\n",
    "\n",
    "Github repo: https://github.com/araffin/rl-tutorial-jnrr19/tree/sb3/\n",
    "\n",
    "Stable-Baselines3: https://github.com/DLR-RM/stable-baselines3\n",
    "\n",
    "Documentation: https://stable-baselines3.readthedocs.io/en/master/\n",
    "\n",
    "SB3-Contrib: https://github.com/Stable-Baselines-Team/stable-baselines3-contrib\n",
    "\n",
    "RL Baselines3 zoo: https://github.com/DLR-RM/rl-baselines3-zoo\n",
    "\n",
    "[RL Baselines3 Zoo](https://github.com/DLR-RM/rl-baselines3-zoo) is a training framework for Reinforcement Learning (RL), using Stable Baselines3.\n",
    "\n",
    "It provides scripts for training, evaluating agents, tuning hyperparameters, plotting results and recording videos.\n",
    "\n",
    "\n",
    "## Introduction\n",
    "\n",
    "In this notebook, you will learn the basics for using stable baselines library: how to create a RL model, train it and evaluate it. Because all algorithms share the same interface, we will see how simple it is to switch from one algorithm to another.\n",
    "\n",
    "\n",
    "## Install Dependencies and Stable Baselines3 Using Pip\n",
    "\n",
    "List of full dependencies can be found in the [README](https://github.com/DLR-RM/stable-baselines3).\n",
    "\n",
    "\n",
    "```\n",
    "pip install stable-baselines3[extra]\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for autoformatting\n",
    "# %load_ext jupyter_black"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 784
    },
    "colab_type": "code",
    "id": "gWskDE2c9WoN",
    "outputId": "03477445-4249-49c3-ddba-4e12df09e98e"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[33mWarning:\u001b[0m No available formula with the name \"xvfb\".\n",
      "\u001b[34m==>\u001b[0m \u001b[1mSearching for similarly named formulae and casks...\u001b[0m\n",
      "\u001b[31mError:\u001b[0m No formulae or casks found for xvfb.\n"
     ]
    }
   ],
   "source": [
    "!brew install ffmpeg freeglut\n",
    "!brew install Caskroom/cask/xquartz  # For visualization\n",
    "#!apt install ffmpeg freeglut3-dev xvfb  # For visualization\n",
    "#!pip install \"stable-baselines3[extra]>=2.0.0a4\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FtY8FhliLsGm"
   },
   "source": [
    "## Imports"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "gcX8hEcaUpR0"
   },
   "source": [
    "Stable-Baselines3 works on environments that follow the [gym interface](https://stable-baselines3.readthedocs.io/en/master/guide/custom_env.html).\n",
    "You can find a list of available environment [here](https://gymnasium.farama.org/environments/classic_control/).\n",
    "\n",
    "Not all algorithms can work with all action spaces, you can find more in this [recap table](https://stable-baselines3.readthedocs.io/en/master/guide/algos.html)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "BIedd7Pz9sOs"
   },
   "outputs": [],
   "source": [
    "import gymnasium as gym\n",
    "import numpy as np"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "Ae32CtgzTG3R"
   },
   "source": [
    "The first thing you need to import is the RL model, check the documentation to know what you can use on which problem"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "R7tKaBFrTR0a"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3 import PPO"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "-0_8OQbOTTNT"
   },
   "source": [
    "The next thing you need to import is the policy class that will be used to create the networks (for the policy/value functions).\n",
    "This step is optional as you can directly use strings in the constructor: \n",
    "\n",
    "```PPO('MlpPolicy', env)``` instead of ```PPO(MlpPolicy, env)```\n",
    "\n",
    "Note that some algorithms like `SAC` have their own `MlpPolicy`, that's why using string for the policy is the recommended option."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ROUJr675TT01"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.ppo.policies import MlpPolicy"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "RapkYvTXL7Cd"
   },
   "source": [
    "## Create the Gym env and instantiate the agent\n",
    "\n",
    "For this example, we will use CartPole environment, a classic control problem.\n",
    "\n",
    "\"A pole is attached by an un-actuated joint to a cart, which moves along a frictionless track. The system is controlled by applying a force of +1 or -1 to the cart. The pendulum starts upright, and the goal is to prevent it from falling over. A reward of +1 is provided for every timestep that the pole remains upright. \"\n",
    "\n",
    "Cartpole environment: [https://gymnasium.farama.org/environments/classic_control/cart_pole/](https://gymnasium.farama.org/environments/classic_control/cart_pole/)\n",
    "\n",
    "![Cartpole](https://cdn-images-1.medium.com/max/1143/1*h4WTQNVIsvMXJTCpXm_TAw.gif)\n",
    "\n",
    "\n",
    "We chose the MlpPolicy because the observation of the CartPole task is a feature vector, not images.\n",
    "\n",
    "The type of action to use (discrete/continuous) will be automatically deduced from the environment action space\n",
    "\n",
    "Here we are using the [Proximal Policy Optimization](https://stable-baselines3.readthedocs.io/en/master/modules/ppo.html) algorithm, which is an Actor-Critic method: it uses a value function to improve the policy gradient descent (by reducing the variance).\n",
    "\n",
    "It combines ideas from [A2C](https://stable-baselines3.readthedocs.io/en/master/modules/a2c.html) (having multiple workers and using an entropy bonus for exploration) and [TRPO](https://stable-baselines.readthedocs.io/en/master/modules/trpo.html) (it uses a trust region to improve stability and avoid catastrophic drops in performance).\n",
    "\n",
    "PPO is an on-policy algorithm, which means that the trajectories used to update the networks must be collected using the latest policy.\n",
    "It is usually less sample efficient than off-policy alorithms like [DQN](https://stable-baselines.readthedocs.io/en/master/modules/dqn.html), [SAC](https://stable-baselines3.readthedocs.io/en/master/modules/sac.html) or [TD3](https://stable-baselines3.readthedocs.io/en/master/modules/td3.html), but is much faster regarding wall-clock time.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "pUWGZp3i9wyf"
   },
   "outputs": [],
   "source": [
    "env = gym.make(\"CartPole-v1\")\n",
    "\n",
    "model = PPO(MlpPolicy, env, verbose=0)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "4efFdrQ7MBvl"
   },
   "source": [
    "We create a helper function to evaluate the agent:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "63M8mSKR-6Zt"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.base_class import BaseAlgorithm\n",
    "\n",
    "\n",
    "def evaluate(\n",
    "    model: BaseAlgorithm,\n",
    "    num_episodes: int = 100,\n",
    "    deterministic: bool = True,\n",
    ") -> float:\n",
    "    \"\"\"\n",
    "    Evaluate an RL agent for `num_episodes`.\n",
    "\n",
    "    :param model: the RL Agent\n",
    "    :param env: the gym Environment\n",
    "    :param num_episodes: number of episodes to evaluate it\n",
    "    :param deterministic: Whether to use deterministic or stochastic actions\n",
    "    :return: Mean reward for the last `num_episodes`\n",
    "    \"\"\"\n",
    "    # This function will only work for a single environment\n",
    "    vec_env = model.get_env()\n",
    "    obs = vec_env.reset()\n",
    "    all_episode_rewards = []\n",
    "    for _ in range(num_episodes):\n",
    "        episode_rewards = []\n",
    "        done = False\n",
    "        # Note: SB3 VecEnv resets automatically:\n",
    "        # https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecenv-api-vs-gym-api\n",
    "        # obs = vec_env.reset()\n",
    "        while not done:\n",
    "            # _states are only useful when using LSTM policies\n",
    "            # `deterministic` is to use deterministic actions\n",
    "            action, _states = model.predict(obs, deterministic=deterministic)\n",
    "            # here, action, rewards and dones are arrays\n",
    "            # because we are using vectorized env\n",
    "            obs, reward, done, _info = vec_env.step(action)\n",
    "            episode_rewards.append(reward)\n",
    "\n",
    "        all_episode_rewards.append(sum(episode_rewards))\n",
    "\n",
    "    mean_episode_reward = np.mean(all_episode_rewards)\n",
    "    print(f\"Mean reward: {mean_episode_reward:.2f} - Num episodes: {num_episodes}\")\n",
    "\n",
    "    return mean_episode_reward"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "zjEVOIY8NVeK"
   },
   "source": [
    "Let's evaluate the un-trained agent, this should be a random agent."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "xDHLMA6NFk95",
    "outputId": "231b2170-a607-48ed-e9d9-daef596f6384"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Mean reward: 498.98 - Num episodes: 100\n"
     ]
    }
   ],
   "source": [
    "# Random Agent, before training\n",
    "mean_reward_before_train = evaluate(model, num_episodes=100, deterministic=True)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "QjjPxrwkYJ2i"
   },
   "source": [
    "Stable-Baselines already provides you with that helper:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "8z6K9YImYJEx"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.evaluation import evaluate_policy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "4oPTHjxyZSOL"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward: 500.00 +/- 0.00\n"
     ]
    }
   ],
   "source": [
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100, warn=False)\n",
    "\n",
    "print(f\"mean_reward: {mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "r5UoXTZPNdFE"
   },
   "source": [
    "## Train the agent and evaluate it"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "e4cfSXIB-pTF"
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<stable_baselines3.ppo.ppo.PPO at 0x34333be20>"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Train the agent for 10000 steps\n",
    "model.learn(total_timesteps=10_000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "ygl_gVmV_QP7"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/x/devel/drl/stable-baselines3/stable_baselines3/common/evaluation.py:67: UserWarning: Evaluation environment is not wrapped with a ``Monitor`` wrapper. This may result in reporting modified episode lengths and rewards, if other wrappers happen to modify these. Consider wrapping environment first with ``Monitor`` wrapper.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mean_reward:403.01 +/- 108.63\n"
     ]
    }
   ],
   "source": [
    "# Evaluate the trained agent\n",
    "mean_reward, std_reward = evaluate_policy(model, env, n_eval_episodes=100)\n",
    "\n",
    "print(f\"mean_reward:{mean_reward:.2f} +/- {std_reward:.2f}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "A00W6yY3NkHG"
   },
   "source": [
    "Apparently the training went well, the mean reward increased a lot ! "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "xVm9QPNVwKXN"
   },
   "source": [
    "### Prepare video recording"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "MPyfQxD5z26J"
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sh: Xvfb: command not found\n"
     ]
    }
   ],
   "source": [
    "# Set up fake display; otherwise rendering will fail\n",
    "import os\n",
    "os.system(\"Xvfb :1 -screen 0 1024x768x24 &\")\n",
    "os.environ['DISPLAY'] = ':1'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "SLzXxO8VMD6N"
   },
   "outputs": [],
   "source": [
    "import base64\n",
    "from pathlib import Path\n",
    "\n",
    "from IPython import display as ipythondisplay\n",
    "\n",
    "\n",
    "def show_videos(video_path=\"\", prefix=\"\"):\n",
    "    \"\"\"\n",
    "    Taken from https://github.com/eleurent/highway-env\n",
    "\n",
    "    :param video_path: (str) Path to the folder containing videos\n",
    "    :param prefix: (str) Filter the video, showing only the only starting with this prefix\n",
    "    \"\"\"\n",
    "    html = []\n",
    "    for mp4 in Path(video_path).glob(\"{}*.mp4\".format(prefix)):\n",
    "        video_b64 = base64.b64encode(mp4.read_bytes())\n",
    "        html.append(\n",
    "            \"\"\"<video alt=\"{}\" autoplay \n",
    "                    loop controls style=\"height: 400px;\">\n",
    "                    <source src=\"data:video/mp4;base64,{}\" type=\"video/mp4\" />\n",
    "                </video>\"\"\".format(\n",
    "                mp4, video_b64.decode(\"ascii\")\n",
    "            )\n",
    "        )\n",
    "    ipythondisplay.display(ipythondisplay.HTML(data=\"<br>\".join(html)))"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "LTRNUfulOGaF"
   },
   "source": [
    "We will record a video using the [VecVideoRecorder](https://stable-baselines3.readthedocs.io/en/master/guide/vec_envs.html#vecvideorecorder) wrapper, you will learn about those wrapper in the next notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "Trag9dQpOIhx"
   },
   "outputs": [],
   "source": [
    "from stable_baselines3.common.vec_env import VecVideoRecorder, DummyVecEnv\n",
    "\n",
    "\n",
    "def record_video(env_id, model, video_length=500, prefix=\"\", video_folder=\"videos/\"):\n",
    "    \"\"\"\n",
    "    :param env_id: (str)\n",
    "    :param model: (RL model)\n",
    "    :param video_length: (int)\n",
    "    :param prefix: (str)\n",
    "    :param video_folder: (str)\n",
    "    \"\"\"\n",
    "    eval_env = DummyVecEnv([lambda: gym.make(env_id, render_mode=\"rgb_array\")])\n",
    "    # Start the video at step=0 and record 500 steps\n",
    "    eval_env = VecVideoRecorder(\n",
    "        eval_env,\n",
    "        video_folder=video_folder,\n",
    "        record_video_trigger=lambda step: step == 0,\n",
    "        video_length=video_length,\n",
    "        name_prefix=prefix,\n",
    "    )\n",
    "\n",
    "    obs = eval_env.reset()\n",
    "    for _ in range(video_length):\n",
    "        action, _ = model.predict(obs)\n",
    "        obs, _, _, _ = eval_env.step(action)\n",
    "\n",
    "    # Close the video recorder\n",
    "    eval_env.close()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "KOObbeu5MMlR"
   },
   "source": [
    "### Visualize trained agent\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 35
    },
    "colab_type": "code",
    "id": "iATu7AiyMQW2",
    "outputId": "68acb027-6c94-4389-8456-2cfb11494814"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving video to /Users/x/devel/drl/stable-baselines3/rl-tutorial-jnrr19/videos/ppo-cartpole-step-0-to-step-500.mp4\n",
      "Moviepy - Building video /Users/x/devel/drl/stable-baselines3/rl-tutorial-jnrr19/videos/ppo-cartpole-step-0-to-step-500.mp4.\n",
      "Moviepy - Writing video /Users/x/devel/drl/stable-baselines3/rl-tutorial-jnrr19/videos/ppo-cartpole-step-0-to-step-500.mp4\n",
      "\n"
     ]
    },
    {
     "ename": "TypeError",
     "evalue": "must be real number, not NoneType",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mrecord_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mCartPole-v1\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mmodel\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mvideo_length\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m500\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mprefix\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[38;5;124;43mppo-cartpole\u001b[39;49m\u001b[38;5;124;43m\"\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m, in \u001b[0;36mrecord_video\u001b[0;34m(env_id, model, video_length, prefix, video_folder)\u001b[0m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m _ \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mrange\u001b[39m(video_length):\n\u001b[1;32m     24\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs)\n\u001b[0;32m---> 25\u001b[0m     obs, _, _, _ \u001b[38;5;241m=\u001b[39m \u001b[43meval_env\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep\u001b[49m\u001b[43m(\u001b[49m\u001b[43maction\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     27\u001b[0m \u001b[38;5;66;03m# Close the video recorder\u001b[39;00m\n\u001b[1;32m     28\u001b[0m eval_env\u001b[38;5;241m.\u001b[39mclose()\n",
      "File \u001b[0;32m~/devel/drl/stable-baselines3/stable_baselines3/common/vec_env/base_vec_env.py:206\u001b[0m, in \u001b[0;36mVecEnv.step\u001b[0;34m(self, actions)\u001b[0m\n\u001b[1;32m    199\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    200\u001b[0m \u001b[38;5;124;03mStep the environments with the given action\u001b[39;00m\n\u001b[1;32m    201\u001b[0m \n\u001b[1;32m    202\u001b[0m \u001b[38;5;124;03m:param actions: the action\u001b[39;00m\n\u001b[1;32m    203\u001b[0m \u001b[38;5;124;03m:return: observation, reward, done, information\u001b[39;00m\n\u001b[1;32m    204\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstep_async(actions)\n\u001b[0;32m--> 206\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mstep_wait\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/devel/drl/stable-baselines3/stable_baselines3/common/vec_env/vec_video_recorder.py:96\u001b[0m, in \u001b[0;36mVecVideoRecorder.step_wait\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m     94\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecorded_frames \u001b[38;5;241m>\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_length:\n\u001b[1;32m     95\u001b[0m         \u001b[38;5;28mprint\u001b[39m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mSaving video to \u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mvideo_recorder\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m)\n\u001b[0;32m---> 96\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose_video_recorder\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_video_enabled():\n\u001b[1;32m     98\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mstart_video_recorder()\n",
      "File \u001b[0;32m~/devel/drl/stable-baselines3/stable_baselines3/common/vec_env/vec_video_recorder.py:104\u001b[0m, in \u001b[0;36mVecVideoRecorder.close_video_recorder\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    102\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mclose_video_recorder\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m    103\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording:\n\u001b[0;32m--> 104\u001b[0m         \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mvideo_recorder\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mclose\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    105\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecording \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n\u001b[1;32m    106\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecorded_frames \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m1\u001b[39m\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-3.12-24.4.0-0/envs/sb3/lib/python3.10/site-packages/gymnasium/wrappers/monitoring/video_recorder.py:157\u001b[0m, in \u001b[0;36mVideoRecorder.close\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    155\u001b[0m     clip \u001b[38;5;241m=\u001b[39m ImageSequenceClip(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mrecorded_frames, fps\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mframes_per_sec)\n\u001b[1;32m    156\u001b[0m     moviepy_logger \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdisable_logger \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mbar\u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[0;32m--> 157\u001b[0m     \u001b[43mclip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mwrite_videofile\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpath\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mmoviepy_logger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    158\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    159\u001b[0m     \u001b[38;5;66;03m# No frames captured. Set metadata.\u001b[39;00m\n\u001b[1;32m    160\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-3.12-24.4.0-0/envs/sb3/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m evaldict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(_call_\u001b[38;5;241m=\u001b[39mcaller, _func_\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[1;32m    231\u001b[0m es \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, extra \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(extras):\n\u001b[1;32m    233\u001b[0m     ex \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_e\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m i\n\u001b[1;32m    234\u001b[0m     evaldict[ex] \u001b[38;5;241m=\u001b[39m extra\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-3.12-24.4.0-0/envs/sb3/lib/python3.10/site-packages/moviepy/decorators.py:54\u001b[0m, in \u001b[0;36mrequires_duration\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     52\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mAttribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mduration\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m not set\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     53\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m---> 54\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-3.12-24.4.0-0/envs/sb3/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m evaldict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(_call_\u001b[38;5;241m=\u001b[39mcaller, _func_\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[1;32m    231\u001b[0m es \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, extra \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(extras):\n\u001b[1;32m    233\u001b[0m     ex \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_e\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m i\n\u001b[1;32m    234\u001b[0m     evaldict[ex] \u001b[38;5;241m=\u001b[39m extra\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-3.12-24.4.0-0/envs/sb3/lib/python3.10/site-packages/moviepy/decorators.py:135\u001b[0m, in \u001b[0;36muse_clip_fps_by_default\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m    130\u001b[0m new_a \u001b[38;5;241m=\u001b[39m [fun(arg) \u001b[38;5;28;01mif\u001b[39;00m (name\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfps\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;28;01melse\u001b[39;00m arg\n\u001b[1;32m    131\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (arg, name) \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mzip\u001b[39m(a, names)]\n\u001b[1;32m    132\u001b[0m new_kw \u001b[38;5;241m=\u001b[39m {k: fun(v) \u001b[38;5;28;01mif\u001b[39;00m k\u001b[38;5;241m==\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfps\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01melse\u001b[39;00m v\n\u001b[1;32m    133\u001b[0m          \u001b[38;5;28;01mfor\u001b[39;00m (k,v) \u001b[38;5;129;01min\u001b[39;00m k\u001b[38;5;241m.\u001b[39mitems()}\n\u001b[0;32m--> 135\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_a\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mnew_kw\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-3.12-24.4.0-0/envs/sb3/lib/python3.10/site-packages/decorator.py:232\u001b[0m, in \u001b[0;36mfun\u001b[0;34m(*args, **kw)\u001b[0m\n\u001b[1;32m    230\u001b[0m evaldict \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mdict\u001b[39m(_call_\u001b[38;5;241m=\u001b[39mcaller, _func_\u001b[38;5;241m=\u001b[39mfunc)\n\u001b[1;32m    231\u001b[0m es \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[0;32m--> 232\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m i, extra \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28menumerate\u001b[39m(extras):\n\u001b[1;32m    233\u001b[0m     ex \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_e\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m_\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m i\n\u001b[1;32m    234\u001b[0m     evaldict[ex] \u001b[38;5;241m=\u001b[39m extra\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-3.12-24.4.0-0/envs/sb3/lib/python3.10/site-packages/moviepy/decorators.py:22\u001b[0m, in \u001b[0;36mconvert_masks_to_RGB\u001b[0;34m(f, clip, *a, **k)\u001b[0m\n\u001b[1;32m     20\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m clip\u001b[38;5;241m.\u001b[39mismask:\n\u001b[1;32m     21\u001b[0m     clip \u001b[38;5;241m=\u001b[39m clip\u001b[38;5;241m.\u001b[39mto_RGB()\n\u001b[0;32m---> 22\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mf\u001b[49m\u001b[43m(\u001b[49m\u001b[43mclip\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43ma\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mk\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-3.12-24.4.0-0/envs/sb3/lib/python3.10/site-packages/moviepy/video/VideoClip.py:300\u001b[0m, in \u001b[0;36mVideoClip.write_videofile\u001b[0;34m(self, filename, fps, codec, bitrate, audio, audio_fps, preset, audio_nbytes, audio_codec, audio_bitrate, audio_bufsize, temp_audiofile, rewrite_audio, remove_temp, write_logfile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    292\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m make_audio:\n\u001b[1;32m    293\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39maudio\u001b[38;5;241m.\u001b[39mwrite_audiofile(audiofile, audio_fps,\n\u001b[1;32m    294\u001b[0m                                audio_nbytes, audio_bufsize,\n\u001b[1;32m    295\u001b[0m                                audio_codec, bitrate\u001b[38;5;241m=\u001b[39maudio_bitrate,\n\u001b[1;32m    296\u001b[0m                                write_logfile\u001b[38;5;241m=\u001b[39mwrite_logfile,\n\u001b[1;32m    297\u001b[0m                                verbose\u001b[38;5;241m=\u001b[39mverbose,\n\u001b[1;32m    298\u001b[0m                                logger\u001b[38;5;241m=\u001b[39mlogger)\n\u001b[0;32m--> 300\u001b[0m \u001b[43mffmpeg_write_video\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    301\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mbitrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbitrate\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    302\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mpreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreset\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    303\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mwrite_logfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mwrite_logfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    304\u001b[0m \u001b[43m                   \u001b[49m\u001b[43maudiofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudiofile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    305\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mverbose\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mverbose\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    306\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mffmpeg_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mffmpeg_params\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    307\u001b[0m \u001b[43m                   \u001b[49m\u001b[43mlogger\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogger\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    309\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m remove_temp \u001b[38;5;129;01mand\u001b[39;00m make_audio:\n\u001b[1;32m    310\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m os\u001b[38;5;241m.\u001b[39mpath\u001b[38;5;241m.\u001b[39mexists(audiofile):\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-3.12-24.4.0-0/envs/sb3/lib/python3.10/site-packages/moviepy/video/io/ffmpeg_writer.py:213\u001b[0m, in \u001b[0;36mffmpeg_write_video\u001b[0;34m(clip, filename, fps, codec, bitrate, preset, withmask, write_logfile, audiofile, verbose, threads, ffmpeg_params, logger)\u001b[0m\n\u001b[1;32m    211\u001b[0m     logfile \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    212\u001b[0m logger(message\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mMoviepy - Writing video \u001b[39m\u001b[38;5;132;01m%s\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m filename)\n\u001b[0;32m--> 213\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m \u001b[43mFFMPEG_VideoWriter\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfilename\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclip\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msize\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mcodec\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    214\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mpreset\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mpreset\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mbitrate\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mbitrate\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlogfile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mlogfile\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    215\u001b[0m \u001b[43m                            \u001b[49m\u001b[43maudiofile\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maudiofile\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mthreads\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mthreads\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    216\u001b[0m \u001b[43m                            \u001b[49m\u001b[43mffmpeg_params\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mffmpeg_params\u001b[49m\u001b[43m)\u001b[49m \u001b[38;5;28;01mas\u001b[39;00m writer:\n\u001b[1;32m    218\u001b[0m     nframes \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mint\u001b[39m(clip\u001b[38;5;241m.\u001b[39mduration\u001b[38;5;241m*\u001b[39mfps)\n\u001b[1;32m    220\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m t,frame \u001b[38;5;129;01min\u001b[39;00m clip\u001b[38;5;241m.\u001b[39miter_frames(logger\u001b[38;5;241m=\u001b[39mlogger, with_times\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mTrue\u001b[39;00m,\n\u001b[1;32m    221\u001b[0m                                     fps\u001b[38;5;241m=\u001b[39mfps, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124muint8\u001b[39m\u001b[38;5;124m\"\u001b[39m):\n",
      "File \u001b[0;32m~/.pyenv/versions/miniconda3-3.12-24.4.0-0/envs/sb3/lib/python3.10/site-packages/moviepy/video/io/ffmpeg_writer.py:88\u001b[0m, in \u001b[0;36mFFMPEG_VideoWriter.__init__\u001b[0;34m(self, filename, size, fps, codec, audiofile, preset, bitrate, withmask, logfile, threads, ffmpeg_params)\u001b[0m\n\u001b[1;32m     77\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mext \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mfilename\u001b[38;5;241m.\u001b[39msplit(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m.\u001b[39m\u001b[38;5;124m\"\u001b[39m)[\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m     79\u001b[0m \u001b[38;5;66;03m# order is important\u001b[39;00m\n\u001b[1;32m     80\u001b[0m cmd \u001b[38;5;241m=\u001b[39m [\n\u001b[1;32m     81\u001b[0m     get_setting(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mFFMPEG_BINARY\u001b[39m\u001b[38;5;124m\"\u001b[39m),\n\u001b[1;32m     82\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-y\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     83\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-loglevel\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124merror\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m logfile \u001b[38;5;241m==\u001b[39m sp\u001b[38;5;241m.\u001b[39mPIPE \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124minfo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     84\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-f\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrawvideo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     85\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-vcodec\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrawvideo\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m     86\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-s\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124mx\u001b[39m\u001b[38;5;132;01m%d\u001b[39;00m\u001b[38;5;124m'\u001b[39m \u001b[38;5;241m%\u001b[39m (size[\u001b[38;5;241m0\u001b[39m], size[\u001b[38;5;241m1\u001b[39m]),\n\u001b[1;32m     87\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-pix_fmt\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgba\u001b[39m\u001b[38;5;124m'\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m withmask \u001b[38;5;28;01melse\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mrgb24\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[0;32m---> 88\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-r\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;132;43;01m%.02f\u001b[39;49;00m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m \u001b[49m\u001b[38;5;241;43m%\u001b[39;49m\u001b[43m \u001b[49m\u001b[43mfps\u001b[49m,\n\u001b[1;32m     89\u001b[0m     \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-an\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-i\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     90\u001b[0m ]\n\u001b[1;32m     91\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m audiofile \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     92\u001b[0m     cmd\u001b[38;5;241m.\u001b[39mextend([\n\u001b[1;32m     93\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-i\u001b[39m\u001b[38;5;124m'\u001b[39m, audiofile,\n\u001b[1;32m     94\u001b[0m         \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m-acodec\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcopy\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m     95\u001b[0m     ])\n",
      "\u001b[0;31mTypeError\u001b[0m: must be real number, not NoneType"
     ]
    }
   ],
   "source": [
    "record_video(\"CartPole-v1\", model, video_length=500, prefix=\"ppo-cartpole\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "-n4i-fW3NojZ"
   },
   "outputs": [],
   "source": [
    "show_videos(\"videos\", prefix=\"ppo\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "9Y8zg4V566qD"
   },
   "source": [
    "## Bonus: Train a RL Model in One Line\n",
    "\n",
    "The policy class to use will be inferred and the environment will be automatically created. This works because both are [registered](https://stable-baselines3.readthedocs.io/en/master/guide/quickstart.html)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "iaOPfOrwWEP4"
   },
   "outputs": [],
   "source": [
    "model = PPO('MlpPolicy', \"CartPole-v1\", verbose=1).learn(1000)"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "colab_type": "text",
    "id": "FrI6f5fWnzp-"
   },
   "source": [
    "## Conclusion\n",
    "\n",
    "In this notebook we have seen:\n",
    "- how to define and train a RL model using stable baselines3, it takes only one line of code ;)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {},
    "colab_type": "code",
    "id": "73ji3gbNDkf7"
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "accelerator": "GPU",
  "colab": {
   "collapsed_sections": [],
   "include_colab_link": true,
   "name": "1_getting_started.ipynb",
   "provenance": []
  },
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
